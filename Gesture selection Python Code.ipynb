{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO-xV8ohPpoV"
   },
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Introduction to Face Landmarks Detection & Snap Chat Filter with Mediapipe and Python</font> </center>**\n",
    "\n",
    "<img src='https://google.github.io/mediapipe/images/face_mesh_ar_effects.gif'>\n",
    "\n",
    "\n",
    "Alright, so without further ado, let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (0.8.11)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (3.5.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (1.20.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ShkIVgPpoe"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "Let's start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VK6vh5cCPpog"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaA_baeXPpom"
   },
   "source": [
    "As mentioned Mediapipe's face landmarks detection solution internally uses a face detector to get the required Regions of Interest (faces) from the image. So before going to the facial landmarks detection, let's briefly discuss that face detector first, as  Mediapipe also allows to separately use it.\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Face Detection</font>**\n",
    "\n",
    "The [mediapipe's face detection solution](https://google.github.io/mediapipe/solutions/face_detection.html) is based on [BlazeFace](https://arxiv.org/abs/1907.05047) face detector that uses a very lightweight and highly accurate feature extraction network, that is inspired and modified from [MobileNetV1/V2](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html) and used a detection method similar to [Single Shot MultiBox Detector (SSD)](https://arxiv.org/abs/1512.02325). It is capable of running at a speed of 200-1000+ FPS on flagship devices. For more info, you can check the resources [here](https://google.github.io/mediapipe/solutions/face_detection.html#resources).\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Initialize the Mediapipe Face Detection Model</font>**\n",
    "\n",
    "To use the Mediapipe's Face Detection solution, we will first have to initialize the face detection class using the syntax **`mp.solutions.face_detection`**, and then we will have to call the function **`mp.solutions.face_detection.FaceDetection()`** with the arguments explained below:\n",
    "\n",
    "\n",
    "* **`model_selection`** - It is an integer index `( i.e., 0 or 1 )`. When set to `0`, a short-range model is selected that works best for faces within 2 meters from the camera, and when set to `1`, a full-range model is selected that works best for faces within 5 meters. Its default value is `0`.\n",
    "\n",
    "\n",
    "* **`min_detection_confidence`** - It is the minimum detection confidence between `([0.0, 1.0])` required to consider the face-detection model's prediction successful. Its default value is `0.5` ( i.e., 50% ) which means that all the detections with prediction confidence less than `0.5` are ignored by default. \n",
    "\n",
    "\n",
    "We will also have to initialize the drawing class using the syntax **`mp.solutions.drawing_utils`** which is used to visualize the detection results on the images/frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rhFnH12hPpoo"
   },
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face detection class.\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Setup the face detection function.\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV0k8dfBPpoq"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Read an Image</font>**\n",
    "\n",
    "Now we will use the function [**`cv2.imread()`**](https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56) to read a sample image and then display the image using the [**`matplotlib`**](https://matplotlib.org/stable/index.html) library, after converting it into `RGB` from `BGR` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6ug-337Ppow"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Perform Face Detection</font>**\n",
    "\n",
    "Now to perform the detection on the sample image, we will have to pass the image (in `RGB` format) into the loaded model by using the function **`mp.solutions.face_detection.FaceDetection().process()`** and we will get an object that will have an attribute **`detections`** that contains a list of a bounding box and six key points for each face in the image. The six key points are on the:\n",
    "\n",
    "1. **Right Eye**\n",
    "2. **Left Eye**\n",
    "3. **Nose Tip**\n",
    "4. **Mouth Center** \n",
    "5. **Right Ear Tragion** \n",
    "6. **Left Ear Tragion**\n",
    "\n",
    "After performing the detection, we will display the bounding box coordinates and only the first two key points of each detected face in the image, so that you get more intuition about the format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yj2MQKKPpoz"
   },
   "source": [
    "**Note:** *The bounding boxes are composed of `xmin` and `width` (both normalized to `[0.0, 1.0]` by the image width) and `ymin` and `height` (both normalized to `[0.0, 1.0]` by the image height). Each keypoint is composed of `x` and `y`, which are normalized to `[0.0, 1.0]` by the image width and height respectively.*\n",
    "\n",
    "Now we will draw the detected bounding box(es) and the key points on a copy of the sample image using the function **`mp.solutions.drawing_utils.draw_detection()`** from the class **`mp.solutions.drawing_utils`**, we had initialized earlier and will display the resultant image using the matplotlib library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y_vcc7SPpo1"
   },
   "source": [
    "**Note:** *Although, the detector quite accurately detects the faces, but fails to precisely detect facial key points (landmarks) in some scenarios (e.g. for non-frontal, rotated, or occluded faces) so that is why we will need the Mediapipe's face landmarks detection solution for creating the Snapchat filter that is our main goal.* \n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Face Landmarks Detection</font>**\n",
    "\n",
    "Now, let's move to the facial landmarks detection, we will start by initializing the face landmarks detection model.\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Initialize the Mediapipe Face Landmarks Detection Model</font>**\n",
    "\n",
    "To initialize the Mediapipe's face landmarks detection model, we will have to initialize the face mesh class using the syntax **`mp.solutions.face_mesh`** and then we will have to call the function **`mp.solutions.face_mesh.FaceMesh()`** with the arguments explained below:\n",
    "\n",
    "* **`static_image_mode`** - It is a boolean value that is if set to `False`, the solution treats the input images as a video stream. It will try to detect faces in the first input images, and upon a successful detection further localizes the face landmarks. In subsequent images, once all **`max_num_faces`** faces are detected and the corresponding face landmarks are localized, it simply tracks those landmarks without invoking another detection until it loses track of any of the faces. This reduces latency and is ideal for processing video frames. If set to `True`, face detection runs on every input image, ideal for processing a batch of static, possibly unrelated, images. Its default value is `False`.\n",
    "\n",
    "\n",
    "* **`max_num_faces`** - It is the maximum number of faces to detect. Its default value is `1`.\n",
    "\n",
    "\n",
    "* **`min_detection_confidence`** - It is the minimum detection confidence `([0.0, 1.0])` required to consider the face-detection model's prediction correct. Its default value is `0.5` which means that all the detections with prediction confidence less than 50% are ignored by default. \n",
    "\n",
    "\n",
    "* **`min_tracking_confidence`** - It is the minimum tracking confidence `([0.0, 1.0])` from the landmark-tracking model for the face landmarks to be considered tracked successfully, or otherwise face detection will be invoked automatically on the next input image, so increasing its value increases the robustness, but also increases the latency. It is ignored if **`static_image_mode`** is `True`, where face detection simply runs on every image. Its default value is `0.5`.\n",
    "\n",
    "After that, we will initialize the **`mp.solutions.drawing_styles`** class that will allow us to get different provided drawing styles of the landmarks on the images/frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_OawwpRaPpo3"
   },
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for images.\n",
    "face_mesh_images = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=2,\n",
    "                                         min_detection_confidence=0.5)\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)\n",
    "\n",
    "# Initialize the mediapipe drawing styles class.\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4tolCUxPpo3"
   },
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Perform Face Landmarks Detection</font>**\n",
    "\n",
    "Now to perform the landmarks detection, we will pass the image (in `RGB` format) to the face landmarks detection machine learning pipeline by using the function **`mp.solutions.face_mesh.FaceMesh().process()`** and get a list of **four hundred sixty-eight** facial landmarks for each detected face in the image. Each landmark will have:\n",
    "\n",
    "* **`x`** - It is the landmark x-coordinate normalized to [0.0, 1.0] by the image width.\n",
    "\n",
    "* **`y`** - It is the landmark y-coordinate normalized to [0.0, 1.0] by the image height.\n",
    "\n",
    "* **`z`** - It is the landmark z-coordinate normalized to roughly the same scale as **`x`**. It represents the landmark depth with the center of the head being the origin, and the smaller the value is, the closer the landmark is to the camera. \n",
    "\n",
    "\n",
    "We will display only two landmarks of each eye to get an intuition about the format of output, the ml pipeline outputs an object that has an attribute **`multi_face_landmarks`** that contains the found landmarks coordinates of each face as an element of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIUeD8ynPpo5"
   },
   "source": [
    "**Note:** *The z-coordinate is just the relative distance of the landmark from the center of the head, and this distance increases and decreases depending upon the distance from the camera so that is why it represents the depth of each landmark point.*\n",
    "\n",
    "Now we will draw the detected landmarks on a copy of the sample image using the function **`mp.solutions.drawing_utils.draw_landmarks()`** from the class **`mp.solutions.drawing_utils`**, we had initialized earlier and will display the resultant image. The function **`mp.solutions.drawing_utils.draw_landmarks()`** can take the following arguments.\n",
    "\n",
    "* **`image`** - It is the image in RGB format on which the landmarks are to be drawn.\n",
    "\n",
    "\n",
    "* **`landmark_list`** - It is the normalized landmark list that is to be drawn on the image.\n",
    "\n",
    "\n",
    "*  **`connections`** - It is the list of landmark index tuples that specifies how landmarks to be connected in the drawing. The provided options are; `mp_face_mesh.FACEMESH_FACE_OVAL, mp_face_mesh.FACEMESH_LEFT_EYE, mp_face_mesh.FACEMESH_LEFT_EYEBROW, mp_face_mesh.FACEMESH_LIPS, mp_face_mesh.FACEMESH_RIGHT_EYE, mp_face_mesh.FACEMESH_RIGHT_EYEBROW, mp_face_mesh.FACEMESH_TESSELATION, mp_face_mesh.FACEMESH_CONTOURS`.\n",
    "    \n",
    "    \n",
    "*  **`landmark_drawing_spec`** - It specifies the landmarks' drawing settings such as color, line thickness, and circle radius. It can be set equal to the `mp.solutions.drawing_utils.DrawingSpec(color, thickness, circle_radius))` object.\n",
    "\n",
    "\n",
    "*  **`connection_drawing_spec`** - It specifies the connections' drawing settings such as color and line thickness. It can be either a `mp.solutions.drawing_utils.DrawingSpec` object or a function from the class `mp.solutions.drawing_styles`, the currently provided options for face mesh are; `get_default_face_mesh_contours_style()` ,`get_default_face_mesh_tesselation_style()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvhaFsfWPpo7"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Face Landmarks Detection Function</font>**\n",
    "\n",
    "Now we will put all this together to create a function **`detectFacialLandmarks()`** that will perform face landmarks detection on an image and will visualize the resultant image along with the original image or return the resultant image along with the output of the model depending upon the passed arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dEkvTBKbPpo8"
   },
   "outputs": [],
   "source": [
    "def detectFacialLandmarks(image, face_mesh, display = True):\n",
    "    '''\n",
    "    This function performs facial landmarks detection on an image.\n",
    "    Args:\n",
    "        image:     The input image of person(s) whose facial landmarks needs to be detected.\n",
    "        face_mesh: The face landmarks detection function required to perform the landmarks detection.\n",
    "        display:   A boolean value that is if set to true the function displays the original input image, \n",
    "                   and the output image with the face landmarks drawn and returns nothing.\n",
    "    Returns:\n",
    "        output_image: A copy of input image with face landmarks drawn.\n",
    "        results:      The output of the facial landmarks detection on the input image.\n",
    "    '''\n",
    "    \n",
    "    # Perform the facial landmarks detection on the image, after converting it into RGB format.\n",
    "    results = face_mesh.process(image[:,:,::-1])\n",
    "    \n",
    "    # Create a copy of the input image to draw facial landmarks.\n",
    "    output_image = image[:,:,::-1].copy()\n",
    "    \n",
    "    # Check if facial landmarks in the image are found.\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        # Iterate over the found faces.\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh tesselation\n",
    "            # connections using default face mesh tesselation style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh contours\n",
    "            # connections using default face mesh contours style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "\n",
    "    # Check if the original input image and the output image are specified to be displayed.\n",
    "    if display:\n",
    "        \n",
    "        # Display the original input image and the output image.\n",
    "        plt.figure(figsize=[15,15])\n",
    "        plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image);plt.title(\"Output\");plt.axis('off');\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image in BGR format and results of facial landmarks detection.\n",
    "        return np.ascontiguousarray(output_image[:,:,::-1], dtype=np.uint8), results              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sJ3K3AXPpo9"
   },
   "source": [
    "Now we will utilize the function **`detectFacialLandmarks()`** created above to perform face landmarks detection on a few sample images and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25XQHM0IPpo_"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Face Landmarks Detection on Real-Time Webcam Feed</font>**\n",
    "\n",
    "The results on the images were remarkable, but now we will try the function on a real-time webcam feed. We will also calculate and display the number of frames being updated in one second to get an idea of whether this solution can work in real-time on a CPU or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVR04aTdPppA"
   },
   "source": [
    "Impressive! the solution is fast as well as accurate.\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Face Expression Recognition</font>**\n",
    "\n",
    "Now that we have the detected landmarks, we will use them to recognize the facial expressions of people in the images/videos using the classical techniques. Our recognizor will be capable of identifying the following facial expressions:\n",
    "\n",
    "* **Eyes Opened or Closed** 😳 (*can be used to check <font style=\"color:rgb(255,140,0)\"> **drowsiness**</font>, <font style=\"color:rgb(255,140,0)\">**wink**</font>  or <font style=\"color:rgb(255,140,0)\">**shock expression**</font>*)\n",
    "\n",
    "* **Mouth Opened or Closed** 😱 (*can be used to check <font style=\"color:rgb(255,140,0)\">**yawning**</font>*)\n",
    "\n",
    "For the sake of simplicity, we are only limiting this to two expressions. But if you want, you can easily extend this application to make it capable of identifying more facial expressions just by adding more conditional statements or maybe merging these two conditions. Like for example, eyes and mouth both wide open can represent <font style=\"color:rgb(255,140,0)\">**surprise**</font> expression.\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Create a Function to Calculate Size of a Face Part</font>**\n",
    "\n",
    "First, we will create a function **`getSize()`** that will utilize detected landmarks to calculate the size of a face part. All we will need is to figure out a way to isolate the landmarks of the face part and luckily that can easily be done using the frozenset objects (attributes of the  **`mp.solutions.face_mesh`** class), which contain the required indexes.\n",
    "\n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_FACE_OVAL`** contains indexes of <font style=\"color:rgb(0,200,0)\">**face outline**</font>.\n",
    "</center>\n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_LIPS`** contains indexes of <font style=\"color:rgb(0,200,0)\">**lips**</font>.\n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_LEFT_EYE`** contains indexes of <font style=\"color:rgb(0,200,0)\">**left eye**</font>.\n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_RIGHT_EYE`** contains indexes of <font style=\"color:rgb(0,200,0)\">**right eye**</font>.\n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_LEFT_EYEBROW`** contains indexes of <font style=\"color:rgb(0,200,0)\">**left eyebrow**</font>.\n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_RIGHT_EYEBROW`** contains indexes of <font style=\"color:rgb(0,200,0)\">**right eyebrow**</font>.\n",
    "\n",
    "After retrieving the landmarks of the face part, we will simply pass it to the function [**`cv2.boundingRect()`**](https://docs.opencv.org/4.5.3/d3/dc0/group__imgproc__shape.html#ga103fcbda2f540f3ef1c042d6a9b35ac7) to get the width and height of the face part. The function **`cv2.boundingRect(landmarks)`** returns the coordinates **(`x1`, `y1`, `width`, `height`)** of a bounding box enclosing the object (face part), given the landmarks but we will only need the **`height`** and **`width`** of the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vd7HjwtOPppB"
   },
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculate the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of person(s) whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:     The calculated width of the face part of the face whose landmarks were passed.\n",
    "        height:    The calculated height of the face part of the face whose landmarks were passed.\n",
    "        landmarks: An array of landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list.\n",
    "    INDEXES_LIST = list(itertools.chain(*INDEXES))\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES_LIST:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append([int(face_landmarks.landmark[INDEX].x * image_width),\n",
    "                               int(face_landmarks.landmark[INDEX].y * image_height)])\n",
    "    \n",
    "    # Calculate the width and height of the face part.\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "    # Convert the list of landmarks of the face part into a numpy array.\n",
    "    landmarks = np.array(landmarks)\n",
    "    \n",
    "    # Retrurn the calculated width height and the landmarks of the face part.\n",
    "    return width, height, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5xd0OXRPppB"
   },
   "source": [
    "Now we will create a function **`isOpen()`** that will utilize the **`getSize()`** function we had created above to check whether a face part (e.g. mouth or an eye) of a person is opened or closed.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1RB1ToEHrvvVmIHdxSwwKWcw9190lnBcA' width=300>\n",
    "\n",
    "**Hint:** *The height of an opened mouth or eye will be greater than the height of a closed mouth or eye.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GCfAWPmBPppC"
   },
   "outputs": [],
   "source": [
    "def isOpen(image, face_mesh_results, face_part, threshold=5, display=True):\n",
    "    '''\n",
    "    This function checks whether the an eye or mouth of the person(s) is open, \n",
    "    utilizing its facial landmarks.\n",
    "    Args:\n",
    "        image:             The image of person(s) whose an eye or mouth is to be checked.\n",
    "        face_mesh_results: The output of the facial landmarks detection on the image.\n",
    "        face_part:         The name of the face part that is required to check.\n",
    "        threshold:         The threshold value used to check the isOpen condition.\n",
    "        display:           A boolean value that is if set to true the function displays \n",
    "                           the output image and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image of the person with the face part is opened  or not status written.\n",
    "        status:       A dictionary containing isOpen statuses of the face part of all the \n",
    "                      detected faces.  \n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Create a copy of the input image to write the isOpen status.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Create a dictionary to store the isOpen status of the face part of all the detected faces.\n",
    "    status={}\n",
    "    \n",
    "    # Check if the face part is mouth.\n",
    "    if face_part == 'MOUTH':\n",
    "        \n",
    "        # Get the indexes of the mouth.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LIPS\n",
    "        \n",
    "        # Specify the location to write the is mouth open status.\n",
    "        loc = (10, image_height - image_height//40)\n",
    "        \n",
    "        # Initialize a increment that will be added to the status writing location, \n",
    "        # so that the statuses of two faces donot overlap. \n",
    "        increment=-30\n",
    "        \n",
    "    # Check if the face part is left eye.    \n",
    "    elif face_part == 'LEFT EYE':\n",
    "        \n",
    "        # Get the indexes of the left eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "        \n",
    "        # Specify the location to write the is left eye open status.\n",
    "        loc = (10, 30)\n",
    "        \n",
    "        # Initialize a increment that will be added to the status writing location, \n",
    "        # so that the statuses of two faces donot overlap.\n",
    "        increment=30\n",
    "    \n",
    "    # Check if the face part is right eye.    \n",
    "    elif face_part == 'RIGHT EYE':\n",
    "        \n",
    "        # Get the indexes of the right eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_RIGHT_EYE \n",
    "        \n",
    "        # Specify the location to write the is right eye open status.\n",
    "        loc = (image_width-300, 30)\n",
    "        \n",
    "        # Initialize a increment that will be added to the status writing location, \n",
    "        # so that the statuses of two faces donot overlap.\n",
    "        increment=30\n",
    "    \n",
    "    # Otherwise return nothing.\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Iterate over the found faces.\n",
    "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "        \n",
    "         # Get the height of the face part.\n",
    "        _, height, _ = getSize(image, face_landmarks, INDEXES)\n",
    "        \n",
    "         # Get the height of the whole face.\n",
    "        _, face_height, _ = getSize(image, face_landmarks, mp_face_mesh.FACEMESH_FACE_OVAL)\n",
    "        \n",
    "        # Check if the face part is open.\n",
    "        if (height/face_height)*100 > threshold:\n",
    "            \n",
    "            # Set status of the face part to open.\n",
    "            status[face_no] = 'OPEN'\n",
    "            \n",
    "            # Set color which will be used to write the status to green.\n",
    "            color=(0,255,0)\n",
    "        \n",
    "        # Otherwise.\n",
    "        else:\n",
    "            # Set status of the face part to close.\n",
    "            status[face_no] = 'CLOSE'\n",
    "            \n",
    "            # Set color which will be used to write the status to red.\n",
    "            color=(0,0,255)\n",
    "        \n",
    "        # Write the face part isOpen status on the output image at the appropriate location.\n",
    "        cv2.putText(output_image, f'FACE {face_no+1} {face_part} {status[face_no]}.', \n",
    "                    (loc[0],loc[1]+(face_no*increment)), cv2.FONT_HERSHEY_PLAIN, 1.4, color, 2)\n",
    "                \n",
    "    # Check if the output image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the output image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the isOpen statuses of the face part of each detected face.\n",
    "        return output_image, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx0GGdOGPppC"
   },
   "source": [
    "Now we will utilize the function **`isOpen()`** created above to check the mouth and eyes status on a few sample images and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OegKU9lBPppE"
   },
   "source": [
    "As expected, the results are fascinating!\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Snapchat Filter Controlled by Facial Expressions</font>**\n",
    "\n",
    "Now that we have the face expression recognizer, let's start building a Snapchat filter on top of it, that will be triggered based on the facial expressions of the person in real-time. \n",
    "\n",
    "Currently, our face expression recognizer can check whether the eyes and mouth are open `😯` or not `😌` so to get the most out of it, we can overlay scalable eyes `👀` images on top of the eyes of the user when his eyes are open and a video of fire `🔥`  coming out of the mouth of the user when the mouth is open.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1eVhSoBBXmIdhYRgqx9bRXfIco-zuiqcX'>\n",
    "\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Create a Function to Overlay the Image Filters</font>**\n",
    "\n",
    "Now we will create a function **`overlay()`** that will apply the filters on top of the eyes and mouth of a person in images/videos utilizing the facial landmarks to locate the face parts and will also resize the filter images according to the size of the face part on which the filter images will be overlayed.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1-aauJnn8h5WMJ2KSO4CCSBnvGdgBQr4r'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Wwsi3urvPppE"
   },
   "outputs": [],
   "source": [
    "def overlay(image, filter_img, face_landmarks, face_part, INDEXES, display=True):\n",
    "    '''\n",
    "    This function will overlay a filter image over a face part of a person in the image/frame.\n",
    "    Args:\n",
    "        image:          The image of a person on which the filter image will be overlayed.\n",
    "        filter_img:     The filter image that is needed to be overlayed on the image of the person.\n",
    "        face_landmarks: The facial landmarks of the person in the image.\n",
    "        face_part:      The name of the face part on which the filter image will be overlayed.\n",
    "        INDEXES:        The indexes of landmarks of the face part.\n",
    "        display:        A boolean value that is if set to true the function displays \n",
    "                        the annotated image and returns nothing.\n",
    "    Returns:\n",
    "        annotated_image: The image with the overlayed filter on the top of the specified face part.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the image to overlay filter image on.\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # Errors can come when it resizes the filter image to a too small or a too large size .\n",
    "    # So use a try block to avoid application crashing.\n",
    "    try:\n",
    "    \n",
    "        # Get the width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "\n",
    "        # Get the height of the face part on which we will overlay the filter image.\n",
    "        _, face_part_height, landmarks = getSize(image, face_landmarks, INDEXES)\n",
    "        \n",
    "        # Specify the height to which the filter image is required to be resized.\n",
    "        required_height = int(face_part_height*3)\n",
    "        \n",
    "        # Resize the filter image to the required height, while keeping the aspect ratio constant. \n",
    "        resized_filter_img = cv2.resize(filter_img, (int(filter_img_width*\n",
    "                                                         (required_height/filter_img_height)),\n",
    "                                                     required_height))\n",
    "        \n",
    "        # Get the new width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = resized_filter_img.shape\n",
    "\n",
    "        # Convert the image to grayscale and apply the threshold to get the mask image.\n",
    "        _, filter_img_mask = cv2.threshold(cv2.cvtColor(resized_filter_img, cv2.COLOR_BGR2GRAY),\n",
    "                                           25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        # Calculate the center of the face part.\n",
    "        center = landmarks.mean(axis=0).astype(\"int\")\n",
    "\n",
    "        # Check if the face part is mouth.\n",
    "        if face_part == 'MOUTH':\n",
    "\n",
    "            # Calculate the location where the smoke filter will be placed.  \n",
    "            location = (int(center[0] - filter_img_width / 3), int(center[1]))\n",
    "\n",
    "        # Otherwise if the face part is an eye.\n",
    "        else:\n",
    "\n",
    "            # Calculate the location where the eye filter image will be placed.  \n",
    "            location = (int(center[0]-filter_img_width/2), int(center[1]-filter_img_height/2))\n",
    "\n",
    "        # Retrieve the region of interest from the image where the filter image will be placed.\n",
    "        ROI = image[location[1]: location[1] + filter_img_height,\n",
    "                    location[0]: location[0] + filter_img_width]\n",
    "\n",
    "        # Perform Bitwise-AND operation. This will set the pixel values of the region where,\n",
    "        # filter image will be placed to zero.\n",
    "        resultant_image = cv2.bitwise_and(ROI, ROI, mask=filter_img_mask)\n",
    "\n",
    "        # Add the resultant image and the resized filter image.\n",
    "        # This will update the pixel values of the resultant image at the indexes where \n",
    "        # pixel values are zero, to the pixel values of the filter image.\n",
    "        resultant_image = cv2.add(resultant_image, resized_filter_img)\n",
    "\n",
    "        # Update the image's region of interest with resultant image.\n",
    "        annotated_image[location[1]: location[1] + filter_img_height,\n",
    "                        location[0]: location[0] + filter_img_width] = resultant_image\n",
    "            \n",
    "    # Catch and handle the error(s).\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Check if the annotated image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the annotated image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(annotated_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "            \n",
    "        # Return the annotated image.\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_HjhrMpPppF"
   },
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Snapchat Filter on Real-Time Webcam Feed</font>**\n",
    "\n",
    "Now we will utilize the function **`overlay()`** created above to apply filters based on the facial expressions, that we will recognize utilizing the function **`isOpen()`** on a real-time webcam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlayMouth(image, filter_img, face_landmarks, face_part, INDEXES, display=True):\n",
    "    '''\n",
    "    This function will overlay a filter image over a face part of a person in the image/frame.\n",
    "    Args:\n",
    "        image:          The image of a person on which the filter image will be overlayed.\n",
    "        filter_img:     The filter image that is needed to be overlayed on the image of the person.\n",
    "        face_landmarks: The facial landmarks of the person in the image.\n",
    "        face_part:      The name of the face part on which the filter image will be overlayed.\n",
    "        INDEXES:        The indexes of landmarks of the face part.\n",
    "        display:        A boolean value that is if set to true the function displays \n",
    "                        the annotated image and returns nothing.\n",
    "    Returns:\n",
    "        annotated_image: The image with the overlayed filter on the top of the specified face part.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the image to overlay filter image on.\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # Errors can come when it resizes the filter image to a too small or a too large size .\n",
    "    # So use a try block to avoid application crashing.\n",
    "    try:\n",
    "    \n",
    "        # Get the width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "\n",
    "        # Get the height of the face part on which we will overlay the filter image.\n",
    "        _, face_part_height, landmarks = getSize(image, face_landmarks, INDEXES)\n",
    "        \n",
    "        # Specify the height to which the filter image is required to be resized.\n",
    "        required_height = int(face_part_height*6.5)\n",
    "        \n",
    "        # Resize the filter image to the required height, while keeping the aspect ratio constant. \n",
    "        resized_filter_img = cv2.resize(filter_img, (int(filter_img_width*\n",
    "                                                         (required_height/filter_img_height)),\n",
    "                                                     required_height))\n",
    "        \n",
    "        # Get the new width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = resized_filter_img.shape\n",
    "\n",
    "        # Convert the image to grayscale and apply the threshold to get the mask image.\n",
    "        _, filter_img_mask = cv2.threshold(cv2.cvtColor(resized_filter_img, cv2.COLOR_BGR2GRAY),\n",
    "                                           25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        # Calculate the center of the face part.\n",
    "        center = landmarks.mean(axis=0).astype(\"int\")\n",
    "\n",
    "        # Check if the face part is mouth.\n",
    "        if face_part == 'MOUTH':\n",
    "\n",
    "            # Calculate the location where the smoke filter will be placed.  \n",
    "            location = (int((center[0]-25 )- filter_img_width / 3), int(center[1])-120)\n",
    "\n",
    "\n",
    "        # Retrieve the region of interest from the image where the filter image will be placed.\n",
    "        ROI = image[location[1]: location[1] + filter_img_height,\n",
    "                    location[0]: location[0] + filter_img_width]\n",
    "\n",
    "        # Perform Bitwise-AND operation. This will set the pixel values of the region where,\n",
    "        # filter image will be placed to zero.\n",
    "        resultant_image = cv2.bitwise_and(ROI, ROI, mask=filter_img_mask)\n",
    "\n",
    "        # Add the resultant image and the resized filter image.\n",
    "        # This will update the pixel values of the resultant image at the indexes where \n",
    "        # pixel values are zero, to the pixel values of the filter image.\n",
    "        resultant_image = cv2.add(resultant_image, resized_filter_img)\n",
    "\n",
    "        # Update the image's region of interest with resultant image.\n",
    "        annotated_image[location[1]: location[1] + filter_img_height,\n",
    "                        location[0]: location[0] + filter_img_width] = resultant_image\n",
    "            \n",
    "    # Catch and handle the error(s).\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Check if the annotated image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the annotated image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(annotated_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "            \n",
    "        # Return the annotated image.\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def Mask():\n",
    "    #camera_video = cap\n",
    "\n",
    "    camera_video1=cv2.VideoCapture(0)\n",
    "    camera_video1.set(3,1280)\n",
    "    camera_video1.set(4,720)\n",
    "    \n",
    "    # Create named window for resizing purposes.\n",
    "    cv2.namedWindow('Face Landmarks Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Initialize a variable to store the time of the previous frame.\n",
    "    time1 = 0\n",
    "\n",
    "    # Iterate until the webcam is accessed successfully.\n",
    "    while camera_video1.isOpened():\n",
    "\n",
    "        # Read a frame.\n",
    "        ok, frame = camera_video1.read()\n",
    "\n",
    "        # Check if frame is not read properly then continue to the next iteration to \n",
    "        # read the next frame.\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Perform Face landmarks detection.\n",
    "        frame, _ = detectFacialLandmarks(frame, face_mesh_videos, display=False)\n",
    "\n",
    "        # Set the time for this frame to the current time.\n",
    "        time2 = time.time()\n",
    "\n",
    "        # Check if the difference between the previous and this frame time > 0 to avoid \n",
    "        # division by zero.\n",
    "        if (time2 - time1) > 0:\n",
    "\n",
    "            # Calculate the number of frames per second.\n",
    "            frames_per_second = 1.0 / (time2 - time1)\n",
    "\n",
    "            # Write the calculated number of frames per second on the frame. \n",
    "            cv2.putText(frame, 'FPS: {}'.format(int(frames_per_second)), (10, 30),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "\n",
    "        # Update the previous frame time to this frame time.\n",
    "        # As this frame will become previous frame in next iteration.\n",
    "        time1 = time2\n",
    "\n",
    "        # Display the frame.\n",
    "        cv2.imshow('Face Landmarks Detection', frame)\n",
    "\n",
    "        # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "        k = cv2.waitKey(1) & 0xFF    \n",
    "\n",
    "        # Check if 'ESC' is pressed and break the loop.\n",
    "        if(k == 27):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture Object and close the windows.                  \n",
    "    camera_video1.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2lV--zlBPppI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def Smoke():\n",
    "    \n",
    "    # Initialize the VideoCapture object to read from the webcam.\n",
    "    #camera_video = cap\n",
    "\n",
    "    camera_video=cv2.VideoCapture(0)\n",
    "    camera_video.set(3,1280)\n",
    "    camera_video.set(4,720)\n",
    "    \n",
    "    \n",
    "    # Read the left and right eyes images.\n",
    "    left_eye = cv2.imread(r\"C:\\Users\\hp\\Downloads\\drive-download-20221108T174128Z-001\\left_eye.png\")\n",
    "    right_eye = cv2.imread(r\"C:\\Users\\hp\\Downloads\\drive-download-20221108T174128Z-001\\right_eye.png\")\n",
    "\n",
    "    # Initialize the VideoCapture object to read from the smoke animation video stored in the disk.\n",
    "    smoke_animation = cv2.VideoCapture(r\"C:\\Users\\hp\\Downloads\\drive-download-20221108T174128Z-001\\smoke_animation.mp4\")\n",
    "\n",
    "    # Set the smoke animation video frame counter to zero.\n",
    "    smoke_frame_counter = 0\n",
    "\n",
    "    # Iterate until the webcam is accessed successfully.\n",
    "    while camera_video.isOpened():\n",
    "\n",
    "        # Read a frame.\n",
    "        ok, frame = camera_video.read()\n",
    "\n",
    "        # Check if frame is not read properly then continue to the next iteration to read\n",
    "        # the next frame.\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        # Read a frame from smoke animation video\n",
    "        _, smoke_frame = smoke_animation.read()\n",
    "\n",
    "        # Increment the smoke animation video frame counter.\n",
    "        smoke_frame_counter += 1\n",
    "\n",
    "        # Check if the current frame is the last frame of the smoke animation video.\n",
    "        if smoke_frame_counter == smoke_animation.get(cv2.CAP_PROP_FRAME_COUNT):     \n",
    "\n",
    "            # Set the current frame position to first frame to restart the video.\n",
    "            smoke_animation.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "            # Set the smoke animation video frame counter to zero.\n",
    "            smoke_frame_counter = 0\n",
    "\n",
    "        # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "        frame = cv2.flip(frame, 1) \n",
    "\n",
    "        # Perform Face landmarks detection.\n",
    "        _, face_mesh_results = detectFacialLandmarks(frame, face_mesh_videos, display=False)\n",
    "\n",
    "        # Check if facial landmarks are found.\n",
    "        if face_mesh_results.multi_face_landmarks:\n",
    "\n",
    "            # Get the mouth isOpen status of the person in the frame.\n",
    "            _, mouth_status = isOpen(frame, face_mesh_results, 'MOUTH', \n",
    "                                         threshold=15, display=False)\n",
    "\n",
    "            # Get the left eye isOpen status of the person in the frame.\n",
    "            _, left_eye_status = isOpen(frame, face_mesh_results, 'LEFT EYE', \n",
    "                                            threshold=4.5 , display=False)\n",
    "\n",
    "            # Get the right eye isOpen status of the person in the frame.\n",
    "            _, right_eye_status = isOpen(frame, face_mesh_results, 'RIGHT EYE', \n",
    "                                             threshold=4.5, display=False)\n",
    "\n",
    "            # Iterate over the found faces.\n",
    "            for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "\n",
    "                # Check if the left eye of the face is open.\n",
    "                if left_eye_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Overlay the left eye image on the frame at the appropriate location.\n",
    "                    frame = overlay(frame, left_eye, face_landmarks,\n",
    "                                    'LEFT EYE', mp_face_mesh.FACEMESH_LEFT_EYE, display=False)\n",
    "\n",
    "                # Check if the right eye of the face is open.\n",
    "                if right_eye_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Overlay the right eye image on the frame at the appropriate location.\n",
    "                    frame = overlay(frame, right_eye, face_landmarks,\n",
    "                                    'RIGHT EYE', mp_face_mesh.FACEMESH_RIGHT_EYE, display=False)\n",
    "\n",
    "                # Check if the mouth of the face is open.\n",
    "                if mouth_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Overlay the smoke animation on the frame at the appropriate location.\n",
    "                    frame = overlay(frame, smoke_frame, face_landmarks, \n",
    "                                    'MOUTH', mp_face_mesh.FACEMESH_LIPS, display=False)\n",
    "\n",
    "        # Display the frame.\n",
    "        cv2.imshow('Smoke Face Filter', frame)\n",
    "\n",
    "        # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "        k = cv2.waitKey(1) & 0xFF    \n",
    "\n",
    "        # Check if 'ESC' is pressed and break the loop.\n",
    "        if(k == 27):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture Object and close the windows.                  \n",
    "    camera_video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def Mouth():\n",
    "    \n",
    "    #camera_video = cap\n",
    "    camera_video=cv2.VideoCapture(0)\n",
    "    camera_video.set(3,1280)\n",
    "    camera_video.set(4,720)\n",
    "    \n",
    "    # Create named window for resizing purposes.\n",
    "    cv2.namedWindow('Mouth Face Filter', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    #Mouth image\n",
    "    mouth=cv2.imread(r\"C:\\Users\\hp\\Downloads\\png-clipart-teeth-mask-horror-jaws-snapchat-filter-icons-logos-emojis-snapchat-filters-thumbnail-PhotoRoom.png\")\n",
    "\n",
    "    # Iterate until the webcam is accessed successfully.\n",
    "    while camera_video.isOpened():\n",
    "\n",
    "        # Read a frame.\n",
    "        ok, frame = camera_video.read()\n",
    "\n",
    "        # Check if frame is not read properly then continue to the next iteration to read\n",
    "        # the next frame.\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Perform Face landmarks detection.\n",
    "        _, face_mesh_results = detectFacialLandmarks(frame, face_mesh_videos, display=False)\n",
    "\n",
    "        if face_mesh_results.multi_face_landmarks:\n",
    "\n",
    "            # Get the mouth isOpen status of the person in the frame.\n",
    "            _, mouth_status = isOpen(frame, face_mesh_results, 'MOUTH', \n",
    "                                         threshold=15, display=False)\n",
    "\n",
    "            for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "\n",
    "               # Check if the mouth of the face is open.\n",
    "                if mouth_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Overlay the smoke animation on the frame at the appropriate location.\n",
    "                    frame = overlayMouth(frame, mouth, face_landmarks, \n",
    "                                    'MOUTH', mp_face_mesh.FACEMESH_LIPS, display=False)\n",
    "\n",
    "        # Display the frame.\n",
    "        cv2.imshow('Mouth Face Filter', frame)\n",
    "\n",
    "        # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "        k = cv2.waitKey(1) & 0xFF    \n",
    "\n",
    "        # Check if 'ESC' is pressed and break the loop.\n",
    "        if(k == 27):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture Object and close the windows.                  \n",
    "    camera_video.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 0]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 0]\n",
      "[1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 0, 1]\n",
      "[1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#Hand tracking module\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import HandTrackingModule as htm\n",
    "\n",
    "camera_video=cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,720)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "brushThickness=15\n",
    "eraserThickness=50\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "detector=htm.handDetector(detectionCon=0.85)\n",
    "\n",
    "\n",
    "while camera_video.isOpened():\n",
    "\n",
    "    # 1. Importing The Images\n",
    "    success,img=camera_video.read()\n",
    "    img=cv2.flip(img,1)\n",
    "\n",
    "    # 2. Finding Hand Landmarks\n",
    "    img=detector.findHands(img)\n",
    "    lmList=detector.findPosition(img,draw=False)\n",
    "\n",
    "    if len(lmList)!=0:\n",
    "        #print(lmList)\n",
    "\n",
    "        #Intialising Index & Middle Finger Tips\n",
    "        x1,y1=lmList[8][1:]\n",
    "        x2,y2=lmList[12][1:]\n",
    "\n",
    "\n",
    "        # 3. Check which fingers are up\n",
    "\n",
    "        fingers=detector.fingersUp()\n",
    "        print(fingers)\n",
    "\n",
    "        # 4. Tool Selection Mode -> 2 fingers (Index & Middle) are UP!\n",
    "        if fingers[0] and (not fingers[1]) and (not fingers[3]) and (not fingers[2]) and (not fingers[4]) :\n",
    "            #cv2.putText(img, \"Mask\", (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "            camera_video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            Mask()\n",
    "            camera_video=cv2.VideoCapture(0)\n",
    "            camera_video.set(3,1280)\n",
    "            camera_video.set(4,720)\n",
    "            success,img=camera_video.read()\n",
    "            img=cv2.flip(img,1)\n",
    "            \n",
    "        if fingers[1] and fingers[4] and (not fingers[2]) and (not fingers[3]) and (not fingers[0]) :\n",
    "            camera_video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            Mouth()\n",
    "            camera_video=cv2.VideoCapture(0)\n",
    "            camera_video.set(3,1280)\n",
    "            camera_video.set(4,720)\n",
    "            success,img=camera_video.read()\n",
    "            img=cv2.flip(img,1)\n",
    "\n",
    "        if fingers[0] and fingers[4] and (not fingers[1]) and (not fingers[2]) and (not fingers[3]):\n",
    "            camera_video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            Smoke()\n",
    "            camera_video=cv2.VideoCapture(0)\n",
    "            camera_video.set(3,1280)\n",
    "            camera_video.set(4,720)\n",
    "            success,img=camera_video.read()\n",
    "            img=cv2.flip(img,1)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Image\",img)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "\n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "    # Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
